{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/kirushi/.conda/envs/guardrails1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'label', 'response'],\n",
       "        num_rows: 7225\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'label', 'response'],\n",
       "        num_rows: 1275\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'label', 'response'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Krooz/Campus_Recruitment_Text\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '[INST] Instruction:\\nWrite an objective overview about the following colleage student based only on the provided structured data in the JSON format.\\nYou should include details and cover the information mentioned in the placement data. The overview should be 100 - 200 words. \\nDon’t make up information. Don\\'t give any additional feedback just represent the given information in the overview. \\nUse a random human name for the student, Dont start with \\'based on the structured data\\'\\n\\nStructured data:\\n{\\n    \"CGPA\": 7.7,\\n    \"Internships\": 1,\\n    \"Projects\": 1,\\n    \"Workshops/Certifications\": 0,\\n    \"AptitudeTestScore\": 69,\\n    \"SoftSkillsRating\": 4.0,\\n    \"ExtracurricularActivities\": \"No\",\\n    \"PlacementTraining\": \"No\",\\n    \"SSC_Marks\": 55,\\n    \"HSC_Marks\": 69\\n}\\n\\n- SSC_Marks denote marks attained by the student in senion secondary school\\n- HSC_Marks denote marks attained by the student in higher seconday school\\n- CGPA is the cummulative GPA attained by the student in his university\\n \\nOverview:\\n[/INST]',\n",
       " 'label': 'NotPlaced',\n",
       " 'response': 'Johnson is a driven and motivated university student with a CGPA of 7.7 in his academic pursuits. He has completed one internship and one project during his time in university. Johnson has not participated in any workshops or certifications, but has demonstrated a strong aptitude with a score on the aptitude test of 69.\\n\\nIn terms of soft skills, Johnson has a solid rating with a score of 4.0. Additionally, Johnson has been actively involved in extracurricular activities, reporting that he takes part in \"No\" extracurricular organizations.\\n\\nDuring placement training, Johnson did not participate. Despite this, he achieved strong grades attained by the student in his senior secondary school and higher secondary school marks of 55 and 69 respectively.\\n\\nOverall, Johnson is a well-rounded student with a strong academic background and a solid set of skills. His experience working in an internship and his participation in projects, combined with his lack of participation in workshops/certifications, suggests that he is an independent and resourceful student who is not afraid to take on challenging tasks. His rating in soft skills, coupled with his active involvement in extracurricular activities, also speaks to his well-rounded personality and character.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/kirushi/.conda/envs/guardrails1/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-02-15 00:17:18.975030: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-15 00:17:19.713645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-15 00:17:23.045560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 574.88it/s]\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:24<00:00, 12.08s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig to quantize the model int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "llm_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_name, device_map='auto', quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# PEFT Config \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 170082304 || all params: 3922153472 || trainable%: 4.336452033664837\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "\n",
    "# get frozen vs trainable model param statistics\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_report(sample):\n",
    "    return f\"\"\"### Instruction: \n",
    "Classify the student into Placed/NotPlaced based on his/her college report details. The report includes marks scored by the student in various courses and extra curricular activities taken by them.\n",
    "\n",
    "### Report:\n",
    "{sample['response']}\n",
    "\n",
    "### Label:\n",
    "{sample['label']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "Classify the student into Placed/NotPlaced based on his/her college report details. The report includes marks scored by the student in various courses and extra curricular activities taken by them.\n",
      "\n",
      "### Report:\n",
      "Johnson is a driven and motivated university student with a CGPA of 7.7 in his academic pursuits. He has completed one internship and one project during his time in university. Johnson has not participated in any workshops or certifications, but has demonstrated a strong aptitude with a score on the aptitude test of 69.\n",
      "\n",
      "In terms of soft skills, Johnson has a solid rating with a score of 4.0. Additionally, Johnson has been actively involved in extracurricular activities, reporting that he takes part in \"No\" extracurricular organizations.\n",
      "\n",
      "During placement training, Johnson did not participate. Despite this, he achieved strong grades attained by the student in his senior secondary school and higher secondary school marks of 55 and 69 respectively.\n",
      "\n",
      "Overall, Johnson is a well-rounded student with a strong academic background and a solid set of skills. His experience working in an internship and his participation in projects, combined with his lack of participation in workshops/certifications, suggests that he is an independent and resourceful student who is not afraid to take on challenging tasks. His rating in soft skills, coupled with his active involvement in extracurricular activities, also speaks to his well-rounded personality and character.\n",
      "\n",
      "### Label:\n",
      "NotPlaced\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classify_report(dataset['train'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model_args = TrainingArguments(\n",
    "    output_dir = \"/dccstor/kirushikesh/mistral_instruct_classify\",\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 6,\n",
    "    warmup_steps = 0.03,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type='constant',\n",
    "    disable_tqdm=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, \n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Supervised Fine-Tuning Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=classify_report,\n",
    "    args=model_args,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/kirushi/.conda/envs/guardrails1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /u/kirushi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# login to HF hub\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "login(os.getenv('HUGGING_FACE_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# fine-tuned model id\n",
    "model_id = \"/dccstor/kirushikesh/mistral_instruct_classify/checkpoint-167\"\n",
    "\n",
    "finetuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████| 680M/680M [00:16<00:00, 42.4MB/s]\n",
      "tokenizer.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 493k/493k [00:00<00:00, 2.63MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Krooz/placement-classification-mistral-7b-instruct-v1/commit/cfbb3e1aff99a1391fa767923cd01134e1c0cffc', commit_message='Upload tokenizer', commit_description='', oid='cfbb3e1aff99a1391fa767923cd01134e1c0cffc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push model and tokenizer to HF hub under your username\n",
    "finetuned_model.push_to_hub(\"Krooz/placement-classification-mistral-7b-instruct-v1\")\n",
    "tokenizer.push_to_hub(\"Krooz/placement-classification-mistral-7b-instruct-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 648/648 [00:00<00:00, 188kB/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:44<00:00, 22.21s/it]\n",
      "adapter_model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 680M/680M [00:14<00:00, 46.7MB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1.47k/1.47k [00:00<00:00, 643kB/s]\n",
      "tokenizer.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 493k/493k [00:00<00:00, 72.3MB/s]\n",
      "tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1.80M/1.80M [00:00<00:00, 24.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 551/551 [00:00<00:00, 313kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# fine-tuned model id\n",
    "model_id = \"Krooz/placement-classification-mistral-7b-instruct-v1\"\n",
    "\n",
    "# load base LLM model, LoRA params and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "Classify the student into Placed/NotPlaced based on his/her college report details. The report includes marks scored by the student in various courses and extra curricular activities taken by them.\n",
      "\n",
      "### Report:\n",
      "John is a college-level student who has demonstrated academic excellence throughout his schooling journey. He has a cumulative GPA of 6.7 in his university, indicating his strong academic abilities. Additionally, he scored 63 on an aptitude test, showcasing his analytical and problem-solving skills. John has also engaged in one project, demonstrating his creativity and practical skills.\n",
      "\n",
      "In terms of extracurricular activities, John is actively involved in a range of areas. He has participated in one project, which showcases his ability to work collaboratively and achieve results independently. However, he has zero internships and zero workshops/certifications, which could have been an area for improvement.\n",
      "\n",
      "In terms of soft skills, John has a rating of 3.8, which suggests that he has strong social and communication capabilities. He has no placement training, meaning he would benefit from gaining hands-on experience in a professional environment.\n",
      "\n",
      "Overall, John has good academic and extracurricular achievements but would benefit from gaining more practical work experience and soft skills training. He has the potential to be an excellent candidate in the future, and it would be beneficial to him to further develop these areas.\n",
      "\n",
      "### Label:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "def classify_report(sample):\n",
    "    return f\"\"\"### Instruction: \n",
    "Classify the student into Placed/NotPlaced based on his/her college report details. The report includes marks scored by the student in various courses and extra curricular activities taken by them.\n",
    "\n",
    "### Report:\n",
    "{sample['response']}\n",
    "\n",
    "### Label:\n",
    "\"\"\"\n",
    "\n",
    "# select random sample\n",
    "sample = dataset['test'][randrange(len(dataset['test']))]\n",
    "\n",
    "# create prompt for inference\n",
    "prompt = classify_report(sample)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# tokenize input text\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.to(device)\n",
    "\n",
    "# inference, 5 outfit combinations make up around 700-750 tokens\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_new_tokens=20, \n",
    "        do_sample=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "John is a college-level student who has demonstrated academic excellence throughout his schooling journey. He has a cumulative GPA of 6.7 in his university, indicating his strong academic abilities. Additionally, he scored 63 on an aptitude test, showcasing his analytical and problem-solving skills. John has also engaged in one project, demonstrating his creativity and practical skills.\n",
      "\n",
      "In terms of extracurricular activities, John is actively involved in a range of areas. He has participated in one project, which showcases his ability to work collaboratively and achieve results independently. However, he has zero internships and zero workshops/certifications, which could have been an area for improvement.\n",
      "\n",
      "In terms of soft skills, John has a rating of 3.8, which suggests that he has strong social and communication capabilities. He has no placement training, meaning he would benefit from gaining hands-on experience in a professional environment.\n",
      "\n",
      "Overall, John has good academic and extracurricular achievements but would benefit from gaining more practical work experience and soft skills training. He has the potential to be an excellent candidate in the future, and it would be beneficial to him to further develop these areas.\n",
      "\n",
      "Ground truth: \n",
      "NotPlaced\n",
      "\n",
      "Generated output: \n",
      "NotPlaced\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decode token ids to text\n",
    "outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# outputs is a list of length num_prompts\n",
    "# parse the completed part\n",
    "output = outputs[0][len(prompt):]\n",
    "\n",
    "print(f\"Context: \\n{sample['response']}\\n\")\n",
    "print(f\"Ground truth: \\n{sample['label']}\\n\")\n",
    "print(f\"Generated output: \\n{output}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
